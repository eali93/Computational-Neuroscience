{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robert-lamprecht/Computational-Neuroscience/blob/main/CompNeuroProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTkN2CDFCWbn",
        "outputId": "11961791-6fd8-43b8-9155-26420e71c3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install Libraries\n",
        "!pip install numpy scipy scikit-learn --quiet\n",
        "# !pip install --upgrade pandas numpy\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.io #enables uploading of .mat files\n",
        "import matplotlib.pyplot as plt\n",
        "# from sklearn.feature_selection import mutual_info_classif #for MI analysis\n",
        "import math\n",
        "from scipy import stats as st\n",
        "from scipy import signal\n",
        "from scipy import interpolate\n",
        "from scipy.io import loadmat\n",
        "print(\"Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "a3JvfSg7KyR1",
        "outputId": "8e498d3a-8ee4-4d9f-bbcb-bdcb93304caf"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 't5.2019.05.08_singleLetters.mat'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 't5.2019.05.08_singleLetters.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4a809a120055>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load .mat files in in Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't5.2019.05.08_singleLetters.mat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load single letters file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmouseTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'computerMouseTemplates.mat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load handwriting file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m    224\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise OSError(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 't5.2019.05.08_singleLetters.mat'"
          ]
        }
      ],
      "source": [
        "# # Load .mat files in in Python\n",
        "# import scipy.io\n",
        "# dat = scipy.io.loadmat('t5.2019.05.08_singleLetters.mat') #load single letters file\n",
        "# mouseTemplate = scipy.io.loadmat('computerMouseTemplates.mat') #load handwriting file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBTbstWXL8KR"
      },
      "outputs": [],
      "source": [
        "# Helper Function\n",
        "def gaussSmooth_fast(timeSeries, width):\n",
        "    if width == 0:\n",
        "        return timeSeries\n",
        "\n",
        "    wingSize = math.ceil(width * 5)\n",
        "    # Range from -wingSize to wingSize inclusive\n",
        "    x_range = np.arange(-wingSize, wingSize + 1)\n",
        "    gKernel = st.norm.pdf(x_range, 0, width)\n",
        "    # In Python, we don't need the conjugate transpose (conj().T)\n",
        "    # since we're working with real values\n",
        "\n",
        "    normFactor = np.cumsum(gKernel)\n",
        "    test = np.vstack((timeSeries, np.zeros((len(gKernel)-1, timeSeries.shape[1]))))\n",
        "\n",
        "    # Apply the filter\n",
        "    Y = signal.lfilter(gKernel, [1], test)\n",
        "\n",
        "    # Division operations (equivalent of bsxfun in MATLAB)\n",
        "    Y[0:len(gKernel)-1, :] = Y[0:len(gKernel)-1, :] / normFactor[0:len(normFactor)-1, np.newaxis]\n",
        "    Y[-(len(gKernel)-1):, :] = Y[-(len(gKernel)-1):, :] / np.flip(normFactor[0:len(normFactor)-1, np.newaxis], axis=0)\n",
        "\n",
        "    # Extract the relevant part (equivalent to the last line in MATLAB)\n",
        "    midpoint = (len(gKernel) - 1) // 2\n",
        "    Y = Y[midpoint:-(len(gKernel)-1-midpoint), :]\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-JERn6JPFLp"
      },
      "outputs": [],
      "source": [
        "# Helper Function\n",
        "def tsne_warp_dist(d1, d2_mat, n_time_bins_per_trial):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    d1 : numpy.ndarray\n",
        "        A 1 x N vector representing a single data point that has been\n",
        "        'unrolled' from a matrix (T x D) into a vector (1 x TD), where T is\n",
        "        the number of time bins and D is the number of neural dimensions.\n",
        "\n",
        "    d2_mat : numpy.ndarray\n",
        "        An M x N matrix, where each row is a data point.\n",
        "\n",
        "    n_time_bins_per_trial : int\n",
        "        Specifies how many time bins (T) are included in each data point.\n",
        "        The number of neural dimensions is then D = N/T.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    warp_dist : numpy.ndarray\n",
        "        An M x 1 vector representing the distance between d1 and each row of d2.\n",
        "    \"\"\"\n",
        "    # affineWarps is a vector of alpha values to consider\n",
        "    affine_warps = np.linspace(0.7, 1.42, 15)\n",
        "\n",
        "    # infer the number of neural dimensions per data point\n",
        "    n_neural_dim = len(d1) // n_time_bins_per_trial\n",
        "\n",
        "    # reshape d1 into a T x D matrix\n",
        "    d1 = d1.reshape(n_time_bins_per_trial, n_neural_dim)\n",
        "\n",
        "    # eDist represents the euclidean distance between d1 and all rows of d2\n",
        "    # for each alpha\n",
        "    e_dist = np.zeros((d2_mat.shape[0], len(affine_warps)))\n",
        "\n",
        "    # now we fill in eDist one entry at a time\n",
        "    for a in range(len(affine_warps)):\n",
        "        # linearly warp d1 using this alpha\n",
        "        x_orig = np.arange(1, d1.shape[0] + 1)\n",
        "        x_interp = np.linspace(1, d1.shape[0], int(affine_warps[a] * d1.shape[0]))\n",
        "        d1_interp = interpolate.interp1d(x_orig, d1, axis=0)(x_interp)\n",
        "\n",
        "        # compute the euclidean distance between the warped d1 and all points in d2\n",
        "        for row_idx in range(d2_mat.shape[0]):\n",
        "            # reshape d2 into a T x D matrix\n",
        "            d2 = d2_mat[row_idx, :].reshape(n_time_bins_per_trial, n_neural_dim)\n",
        "\n",
        "            # compute the euclidean distance, taking care to compute only\n",
        "            # over the relevant time points\n",
        "            if affine_warps[a] > 1:\n",
        "                df = d1_interp[:d1.shape[0], :] - d2\n",
        "            else:\n",
        "                df = d1_interp - d2[:d1_interp.shape[0], :]\n",
        "\n",
        "            e_dist[row_idx, a] = np.mean(df**2)\n",
        "\n",
        "    # the warp distance is defined as the minimum distance over all the\n",
        "    # alphas, which we take here\n",
        "    warp_dist = np.min(e_dist, axis=1)\n",
        "\n",
        "    return warp_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHx8lIHxbOPL",
        "outputId": "f7927c69-ea05-454c-85b9-daa543070655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# to load files from google drive directly, if have shitty laptop\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "5EKhl-tRBK5K",
        "outputId": "8f5dbc25-45fc-4fbe-8af0-788ce6814c79"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (27072,) into shape (27264,)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-001792f19eaf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Apply Gaussian smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian_filter1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter_cube\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mc_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (27072,) into shape (27264,)"
          ]
        }
      ],
      "source": [
        "#Time Warping & PCA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Note: the filepath i use here is unique to my google drive, i couldnt figure out how to get\n",
        "# the google drive mount to work on shared folders so i copied the mat files to my drive for access\n",
        "dat = scipy.io.loadmat('/content/drive/MyDrive/Emory_Year_2/COMP NEURO/t5.2019.05.08_singleLetters.mat')\n",
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
        "           'greaterThan','comma','apostrophe','tilde','questionMark']\n",
        "\n",
        "# Normalize the neural activity by blockwise z-scoring\n",
        "for letter in letters:\n",
        "    norm_cube = np.array(dat[f'neuralActivityCube_{letter}'], dtype=np.float32)\n",
        "\n",
        "    t_idx = np.arange(3)\n",
        "    for y in range(9):\n",
        "        mn = np.zeros((3, 1, 192))\n",
        "        mn[0, 0, :] = dat['meansPerBlock'][y, :]\n",
        "        mn[1, 0, :] = dat['meansPerBlock'][y, :]\n",
        "        mn[2, 0, :] = dat['meansPerBlock'][y, :]\n",
        "\n",
        "        sd = np.zeros((1, 1, 192))\n",
        "        sd[0, 0, :] = dat['stdAcrossAllData']\n",
        "\n",
        "        norm_cube[t_idx, :, :] -= mn\n",
        "        norm_cube[t_idx, :, :] /= sd\n",
        "        t_idx += 3\n",
        "\n",
        "    dat[f'neuralActivityCube_{letter}'] = norm_cube\n",
        "\n",
        "# Compute trial-averaged activity for each character\n",
        "all_data = np.zeros((2000, 27264))  # You may want to adjust this size\n",
        "all_spatial = np.zeros((200000, 192))  # You may want to adjust this size\n",
        "all_labels = np.zeros(2000, dtype=int)\n",
        "all_avg = []\n",
        "c_idx = 0\n",
        "spatial_idx = 0\n",
        "\n",
        "for f, letter in enumerate(letters):\n",
        "    letter_cube = np.array(dat[f'neuralActivityCube_{letter}'])\n",
        "    for x in range(letter_cube.shape[0]):\n",
        "        # Apply Gaussian smoothing\n",
        "        row = gaussian_filter1d(letter_cube[x, 60:, :], 3, axis=0)\n",
        "        all_data[c_idx, :] = row.flatten()\n",
        "        all_labels[c_idx] = f\n",
        "        c_idx += 1\n",
        "\n",
        "        # Store spatial data\n",
        "        new_chunk = gaussian_filter1d(letter_cube[x, 60:, :], 5, axis=0)\n",
        "        all_spatial[spatial_idx:(spatial_idx + new_chunk.shape[0]), :] = new_chunk\n",
        "        spatial_idx += new_chunk.shape[0]\n",
        "\n",
        "    # Averaged letter activity\n",
        "    avg_let = np.mean(letter_cube, axis=0)\n",
        "    avg_let = gaussian_filter1d(avg_let[60:, :], 5, axis=0)\n",
        "    all_avg.append(avg_let)\n",
        "\n",
        "# Apply PCA to the trial-averaged activity data\n",
        "all_avg = np.vstack(all_avg)\n",
        "pca = PCA(n_components=15)\n",
        "all_data_pca = pca.fit_transform(all_avg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YyoyierBcW9o",
        "outputId": "ca63b1c7-188b-4480-e49a-eb941012b225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neuralActivityCubea\n",
            "neuralActivityCubeb\n",
            "neuralActivityCubec\n",
            "neuralActivityCubed\n",
            "neuralActivityCubee\n",
            "neuralActivityCubef\n",
            "neuralActivityCubeg\n",
            "neuralActivityCubeh\n",
            "neuralActivityCubei\n",
            "neuralActivityCubej\n",
            "neuralActivityCubek\n",
            "neuralActivityCubel\n",
            "neuralActivityCubem\n",
            "neuralActivityCuben\n",
            "neuralActivityCubeo\n",
            "neuralActivityCubep\n",
            "neuralActivityCubeq\n",
            "neuralActivityCuber\n",
            "neuralActivityCubes\n",
            "neuralActivityCubet\n",
            "neuralActivityCubeu\n",
            "neuralActivityCubev\n",
            "neuralActivityCubew\n",
            "neuralActivityCubex\n",
            "neuralActivityCubey\n",
            "neuralActivityCubez\n",
            "neuralActivityCubegreaterThan\n",
            "neuralActivityCubecomma\n",
            "neuralActivityCubeapostrophe\n",
            "neuralActivityCubetilde\n",
            "neuralActivityCubequestionMark\n"
          ]
        }
      ],
      "source": [
        "#t-SNE and KNN\n",
        "\n",
        "#list of letters\n",
        "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w','x','y','z', 'greaterThan','comma','apostrophe','tilde','questionMark']\n",
        "\n",
        "#normalize the neural activity by blockwise z-scoring\n",
        "for letter in letters:\n",
        "    normCube = np.float32(dat['neuralActivityCube' + letter])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsZtnrGYuvKW"
      },
      "outputs": [],
      "source": [
        "# Helper Function\n",
        "\n",
        "def getHandwritingCharacterDefinitions():\n",
        "  \"\"\"\n",
        "  Returns a dictionary with entries that define the names of each character, its length, and where the pen tip begins.\n",
        "\n",
        "  Returns:\n",
        "      charDef(dict)\n",
        "  \"\"\"\n",
        "\n",
        "  charDef = {}\n",
        "\n",
        "  # Define the list of all 31 characters and their names\n",
        "  charDef['charList'] = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
        "                'greaterThan','comma','apostrophe','tilde','questionMark']\n",
        "  charDef['charListAbbr'] = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
        "                '>',',',\"'\",'~','?']\n",
        "\n",
        "  # Define the length of each character (in # of 10 ms bins) to use for each template.\n",
        "  # These were hand-defined based on visual inspection of the reconstructed pen trajectories.\n",
        "  charDef['charLen'] = np.array([99, 91, 70, 104, 98, 125, 110, 104, 79, 92, 127, 68, 132, 90,\n",
        "                        84, 113, 104, 74, 86, 110, 86, 83, 110, 103, 115, 100, 82, 77, 116, 71, 110]).astype(np.int32)\n",
        "\n",
        "  # For each character, this defines the starting location of the pen tip (0 = bottom of the line, 1 = top)\n",
        "  charDef['penStart'] = [0.25, 1, 0.5, 0.5, 0.25, 1.0, 0.25, 1.0, 0.5, 0.5, 1, 1, 0.5, 0.5, 0.25, 0.5, 0.25, 0.5, 0.5, 1,\n",
        "           0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.25, 1, 0.5, 1]\n",
        "\n",
        "  # Dictionary to convert string representation to character index\n",
        "  charDef['strToCharIdx'] = {}\n",
        "  for x in range(len(charDef['charListAbbr'])):\n",
        "    charDef['strToCharIdx'][charDef['charListAbbr'][x]] = x\n",
        "\n",
        "  return charDef\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1j6bkfaxIGn7",
        "outputId": "2102e7e3-0529-4c46-fd0b-13ddb16394f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ahwillia/affinewarp.git\n",
            "  Cloning https://github.com/ahwillia/affinewarp.git to /tmp/pip-req-build-o2sinm9t\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ahwillia/affinewarp.git /tmp/pip-req-build-o2sinm9t\n",
            "  Resolved https://github.com/ahwillia/affinewarp.git to commit 23f9e643d2e74ad930ef283311c2c14c585eb6b9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (3.10.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (0.60.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (1.6.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from affinewarp==0.2.0) (8.3.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->affinewarp==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->affinewarp==0.2.0) (0.43.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->affinewarp==0.2.0) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->affinewarp==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->affinewarp==0.2.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->affinewarp==0.2.0) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->affinewarp==0.2.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "#Time-warping the single letters data for linear decoding of pen tip velocities\n",
        "\n",
        "# Install and load the updated time-warping package (\"Piecewise Linear Time Warping\")\n",
        "!pip install git+https://github.com/ahwillia/affinewarp.git\n",
        "\n",
        "import affinewarp as aw\n",
        "import scipy.io\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from affinewarp.piecewisewarp import PiecewiseWarping # Piecewise warping is the closest warping function to the TWPCA done in the paper\n",
        "import os\n",
        "\n",
        "dat = scipy.io.loadmat('/content/drive/My Drive/Comp Neuro Project/t5.2019.05.08_singleLetters.mat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "krMDDj2JGVTz"
      },
      "outputs": [],
      "source": [
        "#Time-warping the single letters data cont'd\n",
        "\n",
        "# defines the list of all 31 characters and what to call then\n",
        "charDef = getHandwritingCharacterDefinitions()\n",
        "\n",
        "# Pre-processing and Normalizing\n",
        "# Because baseline firing rates drift over time, we normalize each electrode's firing rate by subtracting its mean firing rate within each block of data (re-centering it).\n",
        "# We also divide by each electrode's standard deviation to normalize the units.\n",
        "\n",
        "for char in charDef['charList']:\n",
        "    neuralCube = dat['neuralActivity_' + char].astype(np.float64)\n",
        "\n",
        "    # get the trials that belong to this character\n",
        "    trlIdx = []\n",
        "    for t in range(dat['characterCues'].shape[0]):\n",
        "      if dat['characterCues'][t,0] == char:\n",
        "        trlIdx.append(t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7k4QnKJXhYA"
      },
      "outputs": [],
      "source": [
        "#reconstruct letter trajectories from neural activity\n",
        "\n",
        "# Noticed that this script actually imports warped data from the \"Step1_timeWarp\" they did - will come back working on this Monday (4/14) afternoon\n",
        "\n",
        "# save the letters\n",
        "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v', 'w', 'x', 'y', 'z',\n",
        "    'greaterThan', 'comma', 'apostrophe', 'tilde', 'questionMark']\n",
        "\n",
        "# fix the 'o' and 'x' templates which don't match T5's writing style\n",
        "\n",
        "## 1. rotation matrix for -90 degrees:\n",
        "theta = -90 * np.pi / 180  # convert to radians\n",
        "rot90 = np.array([[np.cos(theta), np.cos(0)],[np.sin(theta), np.sin(0)]])\n",
        "\n",
        "## apply rotation to the first two columns of the 'o' template\n",
        "\n",
        "mouseTemplates['o'][:, 0:2] = (rot90 @ mouseTemplates['o'][:, 0:2].T).T\n",
        "\n",
        "## 2. set y = -x for indices 22 to 43\n",
        "mouseTemplates['x'][21:43, 1] = -mouseTemplates['x'][21:43, 0]\n",
        "\n",
        "## flip x and y for indices 47 to 68\n",
        "mouseTemplates['x'][46:68, 0:2] = -mouseTemplates['x'][46:68, 0:2]\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
